{"paragraphs":[{"text":"%spark2.pyspark\nspark.version","dateUpdated":"2019-03-21T19:03:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035481_828436907","id":"20190318-194232_716227193","dateCreated":"2019-03-21T19:03:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:425"},{"text":"%sh\nwget wolly.cs.smu.ca/tmp/kmeans_example.csv","user":"anonymous","dateUpdated":"2019-03-21T19:04:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035487_827667410","id":"20190320-001457_876454073","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-21T19:04:53+0000","dateFinished":"2019-03-21T19:04:53+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:426"},{"title":"Plot the data","text":"%spark2.pyspark\nimport matplotlib.pyplot as plt \nfrom numpy import genfromtxt\nX = genfromtxt('kmeans_example.csv', delimiter=',')\nplt.scatter(X[:,0], X[:,1], s = 50, c = 'b')\nplt.show()","dateUpdated":"2019-03-21T19:05:38+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035488_813431700","id":"20190318-194250_423416341","dateCreated":"2019-03-21T19:03:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:427"},{"title":"Copy the data to HDFS","text":"%sh\nhadoop fs -mkdir /__dsets/kmeans\nhadoop fs -put kmeans_example.csv /__dsets/kmeans/","dateUpdated":"2019-03-21T19:05:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/sh","results":{},"enabled":true,"editorSetting":{"language":"sh","editOnDblClick":false},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035489_813046951","id":"20190320-000136_1623911371","dateCreated":"2019-03-21T19:03:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:428"},{"title":"First try: run kmeans as is (will give an error)","text":"%spark2.pyspark\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"/__dsets/kmeans/kmeans_example.csv\")\n\ndf.show(2)\n\nkmeans = KMeans().setK(2).setSeed(1L)\nmodel = kmeans.fit(df)\n\n# Results of clustering\nresults = model.transform(df)\n\nresults.show(2)\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(results)\nprint(silhouette)\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)","user":"anonymous","dateUpdated":"2019-03-22T13:04:22+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035490_814201198","id":"20190320-001820_1864139206","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-22T13:04:23+0000","dateFinished":"2019-03-22T13:04:23+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:429"},{"title":"Second try: convert features to a Vector","text":"%spark2.pyspark\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"/__dsets/kmeans/kmeans_example.csv\")\n\nassembler = VectorAssembler(\n    inputCols=[\"_c0\", \"_c1\"],\n    outputCol=\"features\")\n    \ndf = assembler.transform(df)\n\ndf.show(2)\n\n\nkmeans = KMeans().setK(2).setSeed(1L)\nmodel = kmeans.fit(df)\n\n# Results of clustering\nresults = model.transform(df)\n\n\nresults.show(2)\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(results)\nprint(silhouette)\n\n# Shows the result.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n\n","user":"anonymous","dateUpdated":"2019-03-22T14:00:31+0000","config":{"lineNumbers":true,"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035491_813816449","id":"20190320-185900_6135696","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-22T14:00:31+0000","dateFinished":"2019-03-22T14:00:32+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:430"},{"title":"Plot the results","text":"%spark2.pyspark\npred = results.toPandas()\nimport matplotlib.pyplot as plt \nfrom numpy import genfromtxt\n\nplt.scatter(pred[\"_c0\"], pred[\"_c1\"], s = 50, c = pred[\"prediction\"])\nplt.show()\n","user":"anonymous","dateUpdated":"2019-03-22T13:58:10+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035491_813816449","id":"20190320-010740_471767239","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-22T13:58:10+0000","dateFinished":"2019-03-22T13:58:11+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:431"},{"title":"Exercise 1","text":"%spark2.pyspark\n#try the following number of clusters: 3, 4, 8, 10\n#place clustering results on the plots\n#which one gives the best silhouette score?","user":"anonymous","dateUpdated":"2019-03-21T19:25:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553196214374_-342873373","id":"20190321-192334_1819595034","dateCreated":"2019-03-21T19:23:34+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:432"},{"title":"Third try: use Pipeline","text":"%spark2.pyspark\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").load(\"/__dsets/kmeans/kmeans_example.csv\")\n\nva = VectorAssembler(\n    inputCols=[\"_c0\", \"_c1\"],\n    outputCol=\"features\")\n    \nkmeans = KMeans().setK(2).setSeed(1L)\n\npipeline = Pipeline(stages=[va, kmeans])\n\nmodel = pipeline.fit(df)\n\nres = model.transform(df)\nres.show(2)\n\n# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(res)\nprint(silhouette)\n\n# Shows the result.\ncenters = model.stages[-1].clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n\n","user":"anonymous","dateUpdated":"2019-03-21T19:17:19+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035491_813816449","id":"20190320-193526_1246318686","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-21T19:17:19+0000","dateFinished":"2019-03-21T19:17:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:433"},{"title":"Plot the results","text":"%spark2.pyspark\npred = res.toPandas()\nimport matplotlib.pyplot as plt \nfrom numpy import genfromtxt\n\nplt.scatter(pred[\"_c0\"], pred[\"_c1\"], s = 50, c = pred[\"prediction\"])\nplt.show()\n","user":"anonymous","dateUpdated":"2019-03-21T19:23:03+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true,"editorHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195855029_-1057637783","id":"20190321-191735_512159278","dateCreated":"2019-03-21T19:17:35+0000","dateStarted":"2019-03-21T19:23:03+0000","dateFinished":"2019-03-21T19:23:03+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:434"},{"title":"Classification with Random Forest","text":"%spark2.pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\ndf = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"/__dsets/census/adult.data\")\n#df.printSchema()\n\n#rename the columns: replace the unnecessary spaces and dashes\nfor colname in df.columns:\n    df = df.withColumnRenamed(colname, colname.strip().replace('-', '_'))\n\n\npipe_stages = []\ndf.show(2)\n#index output\nout_indexer = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n#out_indexer.fit(df).transform(df).show(2)\npipe_stages += [out_indexer]\n\n#define columns what will be used for prediction of income in our model\n#age, workclass, education|education_num, marital_status, occupation, race, sex, hours_per_week, native_country\n\n#which of the columns are numeric?\nnumeric = [\"age\", \"hours_per_week\"]\n\n#which of the columns need to be converted to numeric format?\nnominal = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"race\", \"sex\", \"native_country\"]\n\n#create indexer to create index columns\nindexers = []\nfor col in nominal:\n    indexers += [StringIndexer(inputCol=col, outputCol=col + \"Idx\").setHandleInvalid(\"keep\")]\npipe_stages += indexers\n\n#create OneHot encoder to encode indexed columns to vectors     \nin_names = map(lambda col: col + \"Idx\", nominal)\nout_names = map(lambda col: col + \"Vec\", nominal)\nohe = OneHotEncoderEstimator(inputCols=in_names, outputCols=out_names)   \npipe_stages += [ohe]\n\n#now assemble all columns into a feature vector\nassembler = VectorAssembler(inputCols=out_names + numeric, outputCol=\"features\")\npipe_stages += [assembler]\n\n#create a Random Forest classifier\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\npipe_stages += [rf]\n\n#create a pipeline\npipeline = Pipeline(stages=pipe_stages)\n\n#split data into training and test\n(trainingData, testData) = df.randomSplit([0.75, 0.25], 1L)\nmodel = pipeline.fit(trainingData)\n\npred = model.transform(testData)\npred.select(\"label\", \"income\", \"prediction\", \"probability\", \"education\", \"occupation\").show(3)\n#pred.select(\"educationVec\").show(4)\n\n#Evaluate model - gives AUC (the higher value is better)\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(pred)\n\n","user":"anonymous","dateUpdated":"2019-03-23T00:02:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035493_811507956","id":"20190320-192600_397762261","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-23T00:02:23+0000","dateFinished":"2019-03-23T00:02:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:435","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n|age|        workclass| fnlwgt| education|education_num|     marital_status|      occupation|  relationship|  race|  sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+\n| 39|        State-gov|77516.0| Bachelors|         13.0|      Never-married|    Adm-clerical| Not-in-family| White| Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n| 50| Self-emp-not-inc|83311.0| Bachelors|         13.0| Married-civ-spouse| Exec-managerial|       Husband| White| Male|         0.0|         0.0|          13.0| United-States| <=50K|\n+---+-----------------+-------+----------+-------------+-------------------+----------------+--------------+------+-----+------------+------------+--------------+--------------+------+\nonly showing top 2 rows\n\n+-----+------+----------+--------------------+---------+----------+\n|label|income|prediction|         probability|education|occupation|\n+-----+------+----------+--------------------+---------+----------+\n|  0.0| <=50K|       0.0|[0.95937893204513...|     11th|         ?|\n|  0.0| <=50K|       0.0|[0.93595446928226...|     10th|         ?|\n|  0.0| <=50K|       0.0|[0.95937893204513...|     11th|         ?|\n+-----+------+----------+--------------------+---------+----------+\nonly showing top 3 rows\n\n0.8551194759508524\n"}]}},{"title":"Exercise 2","text":"%spark2.pyspark\n#instead of using \"education\" column, use \"education_num\" column as a feature\n#compare AUC. Which option gives better AUC: \"education\" or \"education_num\"?\n\n#try to use SVM Linear instead of Random forest. Which model gives better classification results?\n","dateUpdated":"2019-03-23T00:03:16+0000","config":{"editorSetting":{"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035493_811507956","id":"20190321-010923_1272373059","dateCreated":"2019-03-21T19:03:55+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:436","user":"anonymous","dateFinished":"2019-03-23T00:03:17+0000","dateStarted":"2019-03-23T00:03:17+0000"},{"title":"Model Tuning: paramGrid","text":"%spark2.pyspark\nfrom pyspark.ml.tuning import ParamGridBuilder\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.numTrees, [1, 5])\n             .build())\n\n\n","user":"anonymous","dateUpdated":"2019-03-23T00:03:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035494_812662202","id":"20190320-165909_1225681838","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-23T00:03:27+0000","dateFinished":"2019-03-23T00:03:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:437","results":{"code":"SUCCESS","msg":[]}},{"text":"%spark2.pyspark\nfrom pyspark.ml.tuning import TrainValidationSplit\ntvs = TrainValidationSplit(estimator=pipeline,\n                           estimatorParamMaps=paramGrid,\n                           evaluator=evaluator,\n                           # 75% of the data will be used for training, 25% for validation.\n                           trainRatio=0.75)\n\nmodel = tvs.fit(trainingData)\npredTvs = model.transform(testData)\nevaluator.evaluate(predTvs)","user":"anonymous","dateUpdated":"2019-03-23T00:04:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553298837582_-1518591993","id":"20190322-235357_556250497","dateCreated":"2019-03-22T23:53:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1500","dateFinished":"2019-03-23T00:04:42+0000","dateStarted":"2019-03-23T00:04:19+0000","errorMessage":"","title":"Train-Validation split"},{"text":"%spark2.pyspark\nfrom pyspark.ml.tuning import CrossValidator\ncv = CrossValidator(estimator=pipeline, \n                    estimatorParamMaps=paramGrid, \n                    evaluator=evaluator,\n                    numFolds=5)\n\nmodel = cv.fit(trainingData)\npredCv = model.transform(testData)\nevaluator.evaluate(predCv)","user":"anonymous","dateUpdated":"2019-03-23T00:07:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553298815338_1332550599","id":"20190322-235335_2030460819","dateCreated":"2019-03-22T23:53:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1441","dateFinished":"2019-03-23T00:06:32+0000","dateStarted":"2019-03-23T00:05:08+0000","errorMessage":"","title":"K-fold cross validation"},{"title":"Getting the best model","text":"%spark2.pyspark\nrf_best = model.bestModel.stages[-1]\nprint(rf_best._java_obj.getMaxDepth())\nprint(rf_best._java_obj.getNumTrees())","user":"anonymous","dateUpdated":"2019-03-22T18:10:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553196941105_-26411684","id":"20190321-193541_386238879","dateCreated":"2019-03-21T19:35:41+0000","dateStarted":"2019-03-21T19:42:56+0000","dateFinished":"2019-03-21T19:42:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:438"},{"title":"Exercise 3","text":"%spark2.pyspark\n#try to tune a Linear SVM model by setting the following regParam values: 0.05, 0.1, 1.0","user":"anonymous","dateUpdated":"2019-03-22T18:56:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553198339195_-233386334","id":"20190321-195859_866793251","dateCreated":"2019-03-21T19:58:59+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:439"},{"title":"Association mining example","text":"%spark2.pyspark\nfrom pyspark.ml.fpm import FPGrowth\n\ndf = spark.createDataFrame([\n    (0, [1, 2, 5]),\n    (1, [1, 2, 3, 5]),\n    (2, [2, 1])\n], [\"id\", \"items\"])\n\nfpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.5, minConfidence=0.6)\nmodel = fpGrowth.fit(df)\n\n# Display frequent itemsets.\nmodel.freqItemsets.show()\n\n# Display generated association rules.\nmodel.associationRules.show()\n\n# transform examines the input items against all the association rules and summarize the\n# consequents as prediction\nmodel.transform(df).show()","user":"anonymous","dateUpdated":"2019-03-22T19:03:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"0":{"graph":{"mode":"table","height":718.056,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035494_812662202","id":"20190320-172336_457797261","dateCreated":"2019-03-21T19:03:55+0000","dateStarted":"2019-03-22T18:59:35+0000","dateFinished":"2019-03-22T18:59:36+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:440"},{"text":"%spark2.pyspark\n","dateUpdated":"2019-03-21T19:03:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1553195035494_812662202","id":"20190321-000545_1241842874","dateCreated":"2019-03-21T19:03:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:441"}],"name":"SparkML2019","id":"2E8M4GUKS","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}